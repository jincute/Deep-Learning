{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Solution to IFT6135 Practical Assignment 2</center>\n",
    "\n",
    "### Team Member: \n",
    "\n",
    "Kun Ni(20139672), Yishi Xu(20125387), Jinfang Luo(20111308), Yan Ai(20027063)\n",
    "\n",
    "### Github repo link\n",
    "This file includes the code snipts for Practical queations 1-3.\n",
    "Please check this [github link](https://github.com/ekunnii/ift6135-submission/tree/master/assignment2) for complete codes \n",
    "\n",
    "plain link: https://github.com/ekunnii/ift6135-submission/tree/master/assignment2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanila RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNCell(nn.Module):\n",
    "    '''\n",
    "    a basic RNN cell,\n",
    "    '''\n",
    "\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        \"\"\"\n",
    "        Most parts are copied from torch.nn.RNNCell.\n",
    "        \"\"\"\n",
    "\n",
    "        super(RNNCell, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.weight_ih = nn.Parameter(torch.Tensor(input_size, hidden_size))\n",
    "        self.weight_hh = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
    "        self.bias = nn.Parameter(torch.Tensor(hidden_size))\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"\n",
    "        Initialize parameters following the way proposed in the paper.\n",
    "        \"\"\"\n",
    "        stdv = 1.0 / math.sqrt(self.hidden_size)\n",
    "        for weight in self.parameters():\n",
    "            init.uniform_(weight, -stdv, stdv)\n",
    "\n",
    "    def forward(self, inputs, hx):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            inputs: A (seq_len, batch ,input_size) tensor containing input\n",
    "                features.\n",
    "            hx: the initial hidden\n",
    "\n",
    "        Returns:\n",
    "            outputs: layers outputs\n",
    "            hx: Tensors containing the next hidden.\n",
    "        \"\"\"\n",
    "        # cell ticks trough seq len, and then return the output and  cell states\n",
    "        batch_size = hx.size(0)\n",
    "        bias_batch = self.bias.unsqueeze(0).expand(\n",
    "            batch_size, self.bias.size(0))\n",
    "\n",
    "        max_time = inputs.size(0)\n",
    "\n",
    "        outputs = []\n",
    "        for time in range(max_time):\n",
    "            input_x = inputs[time]\n",
    "\n",
    "            xw = torch.addmm(bias_batch, input_x, self.weight_ih)\n",
    "            hu = torch.mm(hx, self.weight_hh)\n",
    "\n",
    "            hx = self.tanh(hu + xw)\n",
    "\n",
    "            outputs.append(hx)\n",
    "            # # pass the hidden status to next tick\n",
    "\n",
    "        outputs = torch.stack(outputs, 0)\n",
    "        return outputs, hx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement a stacked vanilla RNN with Tanh nonlinearities.\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, emb_size, hidden_size, seq_len, batch_size, vocab_size, num_layers, dp_keep_prob):\n",
    "        \"\"\"\n",
    "        emb_size:     The number of units in the input embeddings\n",
    "        hidden_size:  The number of hidden units per layer\n",
    "        seq_len:      The length of the input sequences\n",
    "        vocab_size:   The number of tokens in the vocabulary (10,000 for Penn TreeBank)\n",
    "        num_layers:   The depth of the stack (i.e. the number of hidden layers at\n",
    "                      each time-step)\n",
    "        dp_keep_prob: The probability of *not* dropping out units in the\n",
    "                      non-recurrent connections.\n",
    "                      Do not apply dropout on recurrent connections.\n",
    "        \"\"\"\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        # TODO ========================\n",
    "        # Initialization of the parameters of the recurrent and fc layers.\n",
    "        # Your implementation should support any number of stacked hidden layers\n",
    "        # (specified by num_layers), use an input embedding layer, and include fully\n",
    "        # connected layers with dropout after each recurrent layer.\n",
    "        # Note: you may use pytorch's nn.Linear, nn.Dropout, and nn.Embedding\n",
    "        # modules, but not recurrent modules.\n",
    "        #\n",
    "        # To create a  number of parameter tensors and/or nn.Modules\n",
    "        # (for the stacked hidden layer), you may need to use nn.ModuleList or the\n",
    "        # provided clones function (as opposed to a regular python list), in order\n",
    "        # for Pytorch to recognize these parameters as belonging to this nn.Module\n",
    "        # and compute their gradients automatically. You're not obligated to use the\n",
    "        # provided clones function.\n",
    "        self.emb_size = emb_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.seq_len = seq_len\n",
    "        self.batch_size = batch_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Check dropout\n",
    "        if not isinstance(dp_keep_prob, numbers.Number) or not 0 <= dp_keep_prob <= 1 or isinstance(dp_keep_prob, bool):\n",
    "            raise ValueError(\n",
    "                \"dropput should be a number in range[0, 1],\"\n",
    "                \"represneting the probability of an element being zeroed\")\n",
    "\n",
    "        if dp_keep_prob > 0 and num_layers == 1:\n",
    "            warnings.warn(\n",
    "                \"dropout options adds dropout after all but last\"\n",
    "                \"recurrent layer, so non-zero dropout expects\"\n",
    "                \"num_layers greater than 1, but got dropout={} and\"\n",
    "                \"num_layers ={}\".format(dp_keep_prob, num_layers))\n",
    "\n",
    "        # Embedding layers\n",
    "        self.emb = nn.Embedding(vocab_size, emb_size)\n",
    "        self.cell_stack = nn.ModuleList([])\n",
    "        for layer in range(num_layers):\n",
    "            layer_input_size = emb_size if layer == 0 else hidden_size\n",
    "            self.cell_stack.append(RNNCell(input_size=layer_input_size,\n",
    "                                           hidden_size=hidden_size))\n",
    "\n",
    "        self.hidden2out = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "        self.dropout_layer = nn.Dropout(1-dp_keep_prob)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.init_weights_uniform()\n",
    "        self.softmax = nn.Softmax(1)\n",
    "\n",
    "    def init_weights_uniform(self):\n",
    "        # TODO ========================\n",
    "        # Initialize all the weights uniformly in the range [-0.1, 0.1]\n",
    "        # and all the biases to 0 (in place)\n",
    "        # initialize the weight of all hidden units\n",
    "\n",
    "        stdv = 0.1\n",
    "        nn.init.uniform_(self.emb.weight, -stdv, stdv)\n",
    "        nn.init.uniform_(self.hidden2out.weight, -stdv, stdv)\n",
    "        nn.init.zeros_(self.hidden2out.bias)\n",
    "\n",
    "        for cell in self.cell_stack:\n",
    "            cell.reset_parameters()\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # TODO ========================\n",
    "        # initialize the hidden states to zero\n",
    "        \"\"\"\n",
    "        This is used for the first mini-batch in an epoch, only.\n",
    "        \"\"\"\n",
    "\n",
    "        # a parameter tensor of shape (self.num_layers, self.batch_size, self.hidden_size)\n",
    "        if torch.cuda.is_available():\n",
    "            device = torch.device(\"cuda\")\n",
    "        else:\n",
    "            device = torch.device(\"cpu\")\n",
    "        return torch.zeros(self.num_layers, self.batch_size, self.hidden_size, device=device)\n",
    "\n",
    "    def forward(self, inputs, hidden, h_grad=None):\n",
    "        # TODO ========================\n",
    "        # Compute the forward pass, using a nested python for loops.\n",
    "        # The outer for loop should iterate over timesteps, and the\n",
    "        # inner for loop should iterate over hidden layers of the stack.\n",
    "        #\n",
    "        # Within these for loops, use the parameter tensors and/or nn.modules you\n",
    "        # created in __init__ to compute the recurrent updates according to the\n",
    "        # equations provided in the .tex of the assignment.\n",
    "        #\n",
    "        # Note that those equations are for a single hidden-layer RNN, not a stacked\n",
    "        # RNN. For a stacked RNN, the hidden states of the l-th layer are used as\n",
    "        # inputs to to the {l+1}-st layer (taking the place of the input sequence).\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            - inputs: A mini-batch of input sequences, composed of integers that\n",
    "                        represent the index of the current token(s) in the vocabulary.\n",
    "                            shape: (seq_len, batch_size)\n",
    "            - hidden: The initial hidden states for every layer of the stacked RNN.\n",
    "                            shape: (num_layers, batch_size, hidden_size)\n",
    "\n",
    "        Returns:\n",
    "            - Logits for the softmax over output tokens at every time-step.\n",
    "                  **Do NOT apply softmax to the outputs!**\n",
    "                  Pytorch's CrossEntropyLoss function (applied in ptb-lm.py) does\n",
    "                  this computation implicitly.\n",
    "                        shape: (seq_len, batch_size, vocab_size)\n",
    "            - The final hidden states for every layer of the stacked RNN.\n",
    "                  These will be used as the initial hidden states for all the\n",
    "                  mini-batches in an epoch, except for the first, where the return\n",
    "                  value of self.init_hidden will be used.\n",
    "                  See the repackage_hiddens function in ptb-lm.py for more details,\n",
    "                  if you are curious.\n",
    "                        shape: (num_layers, batch_size, hidden_size)\n",
    "        \"\"\"\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            device = torch.device(\"cuda\")\n",
    "        else:\n",
    "            device = torch.device(\"cpu\")\n",
    "\n",
    "        # Pass the inputs into embedding\n",
    "        inputs = self.emb(inputs)\n",
    "        # inputs [seq_len, batch_size, input_feature]\n",
    "        inputs = self.dropout_layer(inputs)\n",
    "\n",
    "        layer_output = torch.zeros(\n",
    "            self.seq_len, self.batch_size, self.hidden_size, device=device)\n",
    "\n",
    "        layers_hidden = []\n",
    "\n",
    "        # The right way should be ticks through each layer and then pass it into next layer.\n",
    "        # in the end we will reach output layers.\n",
    "\n",
    "        h_lists = []\n",
    "\n",
    "        for layer_idx, cell in enumerate(self.cell_stack):\n",
    "            hx_layer = hidden[layer_idx]\n",
    "\n",
    "            if layer_idx == 0:\n",
    "                layer_output, hidden_state = cell(\n",
    "                    inputs, hx_layer)\n",
    "            else:\n",
    "                layer_output, hidden_state = cell(\n",
    "                    layer_output, hx_layer)\n",
    "\n",
    "            h_lists.append(layer_output)\n",
    "            layer_output = self.dropout_layer(layer_output)\n",
    "            layers_hidden.append(hidden_state)\n",
    "\n",
    "        hidden = torch.stack(layers_hidden, 0)\n",
    "        logits = self.hidden2out(layer_output)\n",
    "\n",
    "        if h_grad:\n",
    "            return logits.view(self.seq_len, self.batch_size, self.vocab_size), hidden, h_lists\n",
    "        else:\n",
    "            return logits.view(self.seq_len, self.batch_size, self.vocab_size), hidden\n",
    "\n",
    "    def generate(self, input, hidden, generated_seq_len):\n",
    "        # TODO ========================\n",
    "        # Compute the forward pass, as in the self.forward method (above).\n",
    "        # You'll probably want to copy substantial portions of that code here.\n",
    "        #\n",
    "        # We \"seed\" the generation by providing the first inputs.\n",
    "        # Subsequent inputs are generated by sampling from the output distribution,\n",
    "        # as described in the tex (Problem 5.3)\n",
    "        # Unlike for self.forward, you WILL need to apply the softmax activation\n",
    "        # function here in order to compute the parameters of the categorical\n",
    "        # distributions to be sampled from at each time-step.\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            - input: A mini-batch of input tokens (NOT sequences!)\n",
    "                            shape: (batch_size)\n",
    "            - hidden: The initial hidden states for every layer of the stacked RNN.\n",
    "                            shape: (num_layers, batch_size, hidden_size)\n",
    "            - generated_seq_len: The length of the sequence to generate.\n",
    "                           Note that this can be different than the length used\n",
    "                           for training (self.seq_len)\n",
    "        Returns:\n",
    "            - Sampled sequences of tokens\n",
    "                        shape: (generated_seq_len, batch_size)\n",
    "        \"\"\"\n",
    "\n",
    "        input = self.emb(input)\n",
    "        input.unsqueeze_(0)\n",
    "\n",
    "        generated_words = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i in range(generated_seq_len):\n",
    "                for layer_idx, cell in enumerate(self.cell_stack):\n",
    "                    hx_layer = hidden[layer_idx]\n",
    "                    if layer_idx == 0:\n",
    "                        layer_output, hidden_state = cell(\n",
    "                            input, hx_layer)\n",
    "                    else:\n",
    "                        layer_output, hidden_state = cell(\n",
    "                            layer_output, hx_layer)\n",
    "\n",
    "                    hidden[layer_idx] = hidden_state\n",
    "\n",
    "                logits = self.hidden2out(hidden_state)\n",
    "                logits = self.softmax(logits)\n",
    "                words = torch.multinomial(logits, 1).squeeze()\n",
    "                generated_words.append(words)\n",
    "\n",
    "                input = self.emb(words)\n",
    "                input.unsqueeze_(0)\n",
    "        \n",
    "        return torch.transpose(torch.stack(generated_words, 0), 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(nn.Module):  # Implement a stacked GRU RNN\n",
    "    \"\"\"\n",
    "    Follow the same instructions as for RNN (above), but use the equations for\n",
    "    GRU, not Vanilla RNN.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, emb_size, hidden_size, seq_len, batch_size, vocab_size, num_layers, dp_keep_prob):\n",
    "        super(GRU, self).__init__()\n",
    "\n",
    "        self.emb_size = emb_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.seq_len = seq_len\n",
    "        self.batch_size = batch_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.emb = nn.Embedding(vocab_size, emb_size)\n",
    "        self.cell_stack = nn.ModuleList([])\n",
    "        for layer in range(num_layers):\n",
    "            layer_input_size = emb_size if layer == 0 else hidden_size\n",
    "            self.cell_stack.append(GRUCell(input_size=layer_input_size,\n",
    "                                           hidden_size=hidden_size))\n",
    "\n",
    "        self.hidden2out = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "        self.dropout_layer = nn.Dropout(1-dp_keep_prob)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.init_weights_uniform()\n",
    "        self.softmax = nn.Softmax(1)\n",
    "\n",
    "    def init_weights_uniform(self):\n",
    "        # TODO ========================\n",
    "        stdv = 0.1\n",
    "        nn.init.uniform_(self.emb.weight, -stdv, stdv)\n",
    "        nn.init.uniform_(self.hidden2out.weight, -stdv, stdv)\n",
    "        nn.init.zeros_(self.hidden2out.bias)\n",
    "\n",
    "        for cell in self.cell_stack:\n",
    "            cell.reset_parameters()\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # TODO ========================\n",
    "        # a parameter tensor of shape (self.num_layers, self.batch_size, self.hidden_size)\n",
    "        if torch.cuda.is_available():\n",
    "            device = torch.device(\"cuda\")\n",
    "        else:\n",
    "            device = torch.device(\"cpu\")\n",
    "        return torch.zeros(self.num_layers, self.batch_size, self.hidden_size, device=device)\n",
    "\n",
    "    def forward(self, inputs, hidden, h_grad=None):\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            device = torch.device(\"cuda\")\n",
    "        else:\n",
    "            device = torch.device(\"cpu\")\n",
    "\n",
    "        # Pass the inputs into embedding and then\n",
    "        inputs = self.emb(inputs)\n",
    "        #inputs [seq_len, batch_size, input_feature]\n",
    "        inputs = self.dropout_layer(inputs)\n",
    "\n",
    "        layer_output = torch.zeros(\n",
    "            self.seq_len, self.batch_size, self.hidden_size, device=device)\n",
    "\n",
    "        layers_hidden = []\n",
    "        h_lists = []\n",
    "\n",
    "        # The right way should be ticks through each layer and then pass it into next layer.\n",
    "        # in the end we will reach output layers.\n",
    "\n",
    "        for layer_idx, cell in enumerate(self.cell_stack):\n",
    "            hx_layer = hidden[layer_idx]\n",
    "\n",
    "            if layer_idx == 0:\n",
    "                layer_output, hidden_state = cell(\n",
    "                    inputs, hx_layer)\n",
    "            else:\n",
    "                layer_output, hidden_state = cell(\n",
    "                    layer_output, hx_layer)\n",
    "\n",
    "            h_lists.append(layer_output)\n",
    "            layer_output = self.dropout_layer(layer_output)\n",
    "            layers_hidden.append(hidden_state)\n",
    "\n",
    "        hidden = torch.stack(layers_hidden, 0)\n",
    "        logits = self.hidden2out(layer_output)\n",
    "\n",
    "        # the return hidden status is  stack all layers hidden state\n",
    "        if h_grad:\n",
    "            return logits.view(self.seq_len, self.batch_size, self.vocab_size), hidden, h_lists\n",
    "        else:\n",
    "            return logits.view(self.seq_len, self.batch_size, self.vocab_size), hidden\n",
    "\n",
    "    def generate(self, input, hidden, generated_seq_len):\n",
    "\n",
    "        input = self.emb(input)\n",
    "        input.unsqueeze_(0)\n",
    "\n",
    "        generated_words = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i in range(generated_seq_len):\n",
    "                for layer_idx, cell in enumerate(self.cell_stack):\n",
    "                    hx_layer = hidden[layer_idx]\n",
    "                    if layer_idx == 0:\n",
    "                        layer_output, hidden_state = cell(\n",
    "                            input, hx_layer)\n",
    "                    else:\n",
    "                        layer_output, hidden_state = cell(\n",
    "                            layer_output, hx_layer)\n",
    "\n",
    "                    hidden[layer_idx] = hidden_state\n",
    "\n",
    "                logits = self.hidden2out(hidden_state)\n",
    "                logits = self.softmax(logits)\n",
    "                words = torch.multinomial(logits, 1).squeeze()\n",
    "                generated_words.append(words)\n",
    "\n",
    "                input = self.emb(words)\n",
    "                input.unsqueeze_(0)\n",
    "        \n",
    "        return torch.transpose(torch.stack(generated_words, 0), 0, 1)\n",
    "\n",
    "\n",
    "class GRUCell(nn.Module):\n",
    "    '''\n",
    "    a basic GRU cell,\n",
    "    '''\n",
    "\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        \"\"\"\n",
    "        Most parts are copied from torch.nn.GRUCell.\n",
    "        \"\"\"\n",
    "\n",
    "        super(GRUCell, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.weight_ih = nn.Parameter(\n",
    "            torch.Tensor(input_size, 3 * hidden_size))\n",
    "        self.weight_hh = nn.Parameter(\n",
    "            torch.Tensor(hidden_size, 3 * hidden_size))\n",
    "        self.bias = nn.Parameter(torch.Tensor(3 * hidden_size))\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"\n",
    "        Initialize parameters following the way proposed in the paper.\n",
    "        \"\"\"\n",
    "        stdv = 1.0 / math.sqrt(self.hidden_size)\n",
    "        for weight in self.parameters():\n",
    "            init.uniform_(weight, -stdv, stdv)\n",
    "\n",
    "    def forward(self, inputs, hx):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            inputs: A (seq_len, , batch ,input_size) tensor containing input\n",
    "                features.\n",
    "            hx: the initial hidden\n",
    "\n",
    "        Returns:\n",
    "            outputs: layers outputs\n",
    "            hx: Tensors containing the next hidden.\n",
    "        \"\"\"\n",
    "        # cell ticks trough seq len, and then return the output and  cell states\n",
    "        batch_size = hx.size(0)\n",
    "        bias_batch = self.bias.unsqueeze(0).expand(\n",
    "            batch_size, self.bias.size(0))\n",
    "\n",
    "        max_time = inputs.size(0)\n",
    "        outputs = []\n",
    "        for time in range(max_time):\n",
    "            # concat input to save one matrix operation\n",
    "            input_x = inputs[time]\n",
    "\n",
    "            xw = torch.addmm(bias_batch, input_x, self.weight_ih)\n",
    "            hu = torch.mm(hx, self.weight_hh)\n",
    "            xw2 = torch.split(xw, self.hidden_size, 1)\n",
    "            hu2 = torch.split(hu, self.hidden_size, 1)\n",
    "\n",
    "            z = self.sigmoid(xw2[0] + hu2[0])\n",
    "            r = self.sigmoid(xw2[1] + hu2[1])\n",
    "            hx_ = self.tanh(r * hu2[2] + xw2[2])\n",
    "\n",
    "            hx = (1 - z) * hx_ + z * hx\n",
    "\n",
    "            outputs.append(hx)\n",
    "            # # pass the hidden status to next tick\n",
    "\n",
    "        outputs = torch.stack(outputs, 0)\n",
    "        return outputs, hx\n",
    "\n",
    "\n",
    "'''\n",
    "End for GRU_Cell\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, n_heads, n_units, dropout=0.1):\n",
    "        \"\"\"\n",
    "        n_heads: the number of attention heads\n",
    "        n_units: the number of output units\n",
    "        dropout: probability of DROPPING units\n",
    "        \"\"\"\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        # This sets the size of the keys, values, and queries (self.d_k) to all\n",
    "        # be equal to the number of output units divided by the number of heads.\n",
    "        self.d_k = n_units // n_heads\n",
    "        # This requires the number of n_heads to evenly divide n_units.\n",
    "        assert n_units % n_heads == 0, '{} heads are not evenly dividable by {} units'. format(\n",
    "            n_heads, n_units)\n",
    "        self.n_units = n_units\n",
    "        self.n_heads = n_heads\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # TODO: create/initialize any necessary parameters or layers\n",
    "        # Initialize all weights and biases uniformly in the range [-k, k],\n",
    "        # where k is the square root of 1/n_units.\n",
    "        # Note: the only Pytorch modules you are allowed to use are nn.Linear\n",
    "        # and nn.Dropout\n",
    "        # ETA: you can also use softmax\n",
    "\n",
    "        self.q_dense_layer = nn.Linear(n_units, n_units)\n",
    "        self.k_dense_layer = nn.Linear(n_units, n_units)\n",
    "        self.v_dense_layer = nn.Linear(n_units, n_units)\n",
    "\n",
    "        self.output_dense_layer = nn.Linear(n_units, n_units)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.attention_dropout = nn.Dropout(dropout)\n",
    "        self.init_weights_uniform()\n",
    "\n",
    "    def init_weights_uniform(self):\n",
    "        # TODO ========================\n",
    "        stdv = 1.0 / math.sqrt(self.n_units)\n",
    "        nn.init.uniform_(self.q_dense_layer.weight, -stdv, stdv)\n",
    "        nn.init.uniform_(self.k_dense_layer.weight, -stdv, stdv)\n",
    "        nn.init.uniform_(self.v_dense_layer.weight, -stdv, stdv)\n",
    "        nn.init.uniform_(self.output_dense_layer.weight, -stdv, stdv)\n",
    "\n",
    "        nn.init.uniform_(self.q_dense_layer.bias, -stdv, stdv)\n",
    "        nn.init.uniform_(self.k_dense_layer.bias, -stdv, stdv)\n",
    "        nn.init.uniform_(self.v_dense_layer.bias, -stdv, stdv)\n",
    "        nn.init.uniform_(self.output_dense_layer.bias, -stdv, stdv)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        # TODO: implement the masked multi-head attention.\n",
    "        # query, key, and value all have size: (batch_size, seq_len, self.n_units, self.d_k)\n",
    "        # mask has size: (batch_size, seq_len, seq_len)\n",
    "        # As described in the .tex, apply input masking to the softmax\n",
    "        # generating the \"attention values\" (i.e. A_i in the .tex)\n",
    "        # Also apply dropout to the attention values.\n",
    "\n",
    "        # Codes are mostly copied from attention module in tensorflow\n",
    "\n",
    "        # q k v  -> [batch_size, seq_len, self.n_units]\n",
    "        q = self.q_dense_layer(query)\n",
    "        k = self.k_dense_layer(key)\n",
    "        v = self.v_dense_layer(value)\n",
    "\n",
    "        # split q, k, v into diffrent heads and transpose the resulting value\n",
    "        # [batch_size, seq_len, self.n_units] -> [batch_size, seq_len, depths]\n",
    "        # and then stack into [batch_size, n_heads, seq_len, depths]\n",
    "        q = torch.stack(torch.split(q, self.d_k, dim=2), dim=1)\n",
    "        k = torch.stack(torch.split(k, self.d_k, dim=2), dim=1)\n",
    "        v = torch.stack(torch.split(v, self.d_k, dim=2), dim=1)\n",
    "\n",
    "        # Scale q to prevent the dot product between q and k from growing too large\n",
    "        q *= self.d_k ** -0.5\n",
    "\n",
    "        # Calculate dot product attention\n",
    "        logits = torch.matmul(q, k.transpose(2, 3))\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1).repeat(1, self.n_heads, 1, 1).float()\n",
    "            logits = (logits * mask) - 1.e9 * (1 - mask)\n",
    "\n",
    "        # # We compute the softmax. We minus the score with a max for better numerical stability.\n",
    "        # # [batch_size, n_heads, seq_len, seq_len]\n",
    "        # logits = torch.exp(logits - torch.max(logits, -1, keepdim=True)[0])\n",
    "        # weights = logits / torch.sum(logits, dim=-1, keepdim=True)\n",
    "        weights = self.softmax(logits)\n",
    "\n",
    "        weights = self.attention_dropout(weights)\n",
    "        attention_output = torch.matmul(weights, v)\n",
    "        # combine heads\n",
    "        # [batch_size, n_heads, seq_len, depths] -> [batch_size, seq_len, self.n_units]\n",
    "        seq_len = attention_output.size(2)\n",
    "        attention_output = attention_output.transpose(\n",
    "            1, 2).contiguous().view(-1, seq_len, self.n_units)\n",
    "\n",
    "        attention_output = self.output_dense_layer(attention_output)\n",
    "\n",
    "        return attention_output  # size: (batch_size, seq_len, self.n_units)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
